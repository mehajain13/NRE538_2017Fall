---
title: "NRE538_Cental Limit Therom"
author: "Oscar Feng-Hsun Chang"
date: "Week3"
output: 
  html_document:
    code_folding: show
    highlight: textmate
    keep_md: yes
    number_sections: true
    theme: flatly
    toc: yes
    toc_float:
      collapsed: false
      smooth_scroll: true
      toc_depth: 4
---

# Central limit theorem

To demonstrate the [Central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), I found a cool package, [__Lahman__](https://cran.r-project.org/web/packages/Lahman/Lahman.pdf) containing the salaries of each Major League Baseball player from 1871 to 2015. As you can expect, the salary distribution is a highly right skewed distribution. However, if we subsample sufficiently large number of players and calculate the mean. The distribution of those means will follow normal distribution! Magic~  

Let's first load the package and subset the salaries from just one year. Note that, because the salary data is inside the __Lahman__ package, so we have to use `data()` again to fetch it.   

```{r, Lahman, eval=FALSE}
install.packages("Lahman")
```

```{r, library(Lahman)}
library(Lahman)
data("Salaries")
```

Now let's subset the salaries from one single year (say 2015). Do you remember how to do it?

```{r, salary 2015, echo=FALSE}
money15 = subset(Salaries, yearID==2015)
```

```{r, head(salary 2015)}
head(money15)
```

```{r, mean function}
meansVector = function(times, size, dat, varb){
  a = as.numeric(times)
  b = as.numeric(size)
  v = c()
  for(i in 1:a){
    y = sample(dat[,varb],b,replace=TRUE)
    m = mean(y)
    v = c(v,m)
  }
  v
}
```

Here I write a small function, which will return the means of each subsample as a vector. The input of this function is (1) how many times you what to do subsample, (2) in each subsample, how many values you want to take (how many players' salary you want to subsample out), (3) which year you want to do the subsample (in the following case, it's year 2015, but you can subset another year), and (4) which variable you want to do the subsample ( in the following case, it's "salary").  

Let's take a look of the salary distribution in year 2015. 

```{r, sal 2015 distribution}
hist(money15$salary, main = "distribution of salary")
  avg=mean(money15$salary)
  SD=sd(money15$salary)
  abline(v=avg, col="blue")
  legend("topright", legend = c(paste0("mean=", avg), paste0("SD=", SD)),text.col=c("blue", "dark green"))

```

Obviously, it's highly right skewed.

Now let's plot the histogram of 10 subsamples with 100 values in each subsample. 

```{r, 10, 100, echo=FALSE}
means=meansVector(10, 100, money15, "salary")
avg=round(mean(means), 2)
SD=round(sd(means), 2)
hist(means, xlim = c(1000000, 7000000), breaks=10, probability=TRUE,
     main = "10 subsamples with 50 values each")
lines(density(means, na.rm=TRUE), col="red")
  legend("topleft", legend = c(paste0("mean=", avg), paste0("SD=", SD)),text.col=c("blue", "dark green"))
```

Let's gradually increase the number of subsamples but fix the values in each subsample for now. 

```{r, 20-2560, 100 , echo=FALSE}
x=20*2^(c(0, seq(log2(2), log2(128))))
y=100

#par(mfrow=c(length(x),1),mar=rep(2, 4))
for (i in seq(1,length(x),by=1)){
  means=meansVector(x[i], y, money15, "salary")
  avg=round(mean(means), 2)
  SD=round(sd(means), 2)
  if (x[i]<=20){brk=10}
  else {brk=20}
  hist(means, xlim = c(1000000, 7000000), breaks=brk, probability=TRUE, 
       main = paste0(x[i], " subsamples with ", y, " values each"))
  lines(density(means, na.rm=TRUE), col="red")
  legend("topleft", legend = c(paste0("mean=", avg), paste0("SD=", SD)),text.col=c("blue", "dark green"))
}
```

```{r, 20-2560, 100 With sapply, echo=FALSE, eval=FALSE}
# this it just to try sapply function
x=20*2^(seq(log2(2), log2(64)))
sapply(x, function(x){
              means=meansVector(x, 50, money15, "salary")
              avg=round(mean(means), 2)
              SD=round(sd(means), 2)
              hist(means, xlim = c(1000000, 7000000), main = paste0(x, "subsamples with 50 values each"), probability=TRUE)
               lines(density(means, na.rm=TRUE), col="red")
               legend("topleft", legend = c(paste0("mean=", avg), paste0("SD=", SD)),text.col=c("blue", "dark green"))
              }
      )
```

* What do you observe from these series of histograms? What would happen with the increase of the number of subsamples?

How about we fix the number of subsample but increase the value taken in each subsample?  
We start from 1280 subsamples with 10 values in each subsample.

```{r, 1280, 10, echo=FALSE}
means=meansVector(1280, 10, money15, "salary")
avg=round(mean(means), 2)
SD=round(sd(means), 2)
hist(means, xlim = c(1000000, 7000000), breaks=10, probability=TRUE,
     main = "1280 subsamples with 10 values each")
lines(density(means, na.rm=TRUE), col="red")
  legend("topleft", legend = c(paste0("mean=", avg), paste0("SD=", SD)),text.col=c("blue", "dark green"))
```

Let's gradually increase the values taken in each subsample. 

```{r, echo=FALSE}
x=1280
y=20*2^(c(0, seq(log2(2), log2(128))))

for (i in seq(1,length(y),by=1)){
  means=meansVector(x, y[i], money15, "salary")
  avg=round(mean(means), 2)
  SD=round(sd(means), 2)
  hist(means, xlim = c(1000000, 7000000), breaks=brk, probability=TRUE, 
       main = paste0(x, " subsamples with ", y[i], " values each"))
  lines(density(means, na.rm=TRUE), col="red")
  legend("topleft", legend = c(paste0("mean=", avg), paste0("SD=", SD)),text.col=c("blue", "dark green"))
}
```

* What do you observe from these series of histograms? What would happen with the increase of values taken in each subsample?

---------------------------------------------------------------------------------------------------------------------------------

__Exercise 1__

Try to create same series of histograms with players' salaries in 2014.

__Exercise 2__

Now demonstrate central limit theorem with the "mpg (miles/gallon)" variable in the [mtcars (Motor Trend Car Road Tests)](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html) data set. _mtcars_ is another built-in data set in the _base_ package of __R__. You can use the _meansVector_ function I wrote for you to generate a vector of means. 

*hint:*
Make sure that you are able to  
(1) read in (`mtcars`) data,  
(2) plotting the histogram of "mpg" variable  
(3) create a series of histograms with gradually increasing numbers of subsamples but fixed values taken in each subsample  
(4) create a series of histograms with fixed numbers of subsamples but gradually increasing values taken in each subsample

---------------------------------------------------------------------------------------------------------------------------------

```{r, mtcars read-in, eval=FALSE}
data("mtcars")
hist(mtcars$mpg)
```

```{r, mtcars, eval=FALSE}
hist(mtcars$mpg, main = "distribution of mpg", breaks=10)
  avg=mean(mtcars$mpg)
  SD=sd(mtcars$mpg)
  abline(v=avg, col="blue")
  legend("topright", legend = c(paste0("mean=", avg), paste0("SD=", SD)),text.col=c("blue", "dark green"))
# It's a pretty flat distribution (uniform distribution)
```

```{r, 10, 10 @ mtcars, eval=FALSE, echo=FALSE}
# Plot the histogram of 10 subsamples with 10 values in each subsample. 

means=meansVector(10, 10, mtcars, 'mpg')
avg=round(mean(means), 2)
SD=round(sd(means), 2)
hist(means, xlim = c(15, 25), breaks=10, probability=TRUE,
     main = "10 subsamples with 10 values each")
lines(density(means, na.rm=TRUE), col="red")
  legend("topleft", legend = c(paste0("mean=", avg), paste0("SD=", SD)),text.col=c("blue", "dark green"))
```

```{r, 20-2560, 10 @ mtcars, eval=FALSE, echo=FALSE}
# gradually increase the numbers of subsamples from 20-2560

x=20*2^(c(0, seq(log2(2), log2(128))))
y=10

#par(mfrow=c(length(x),1),mar=rep(2, 4))
for (i in seq(1,length(x),by=1)){
  means=meansVector(x[i], y, mtcars, "mpg")
  avg=round(mean(means), 2)
  SD=round(sd(means), 2)
  if (x[i]<=20){brk=10}
  else {brk=20}
  hist(means, xlim = c(15, 25), breaks=brk, probability=TRUE, 
       main = paste0(x[i], " subsamples with ", y, " values each"))
  lines(density(means, na.rm=TRUE), col="red")
  legend("topleft", legend = c(paste0("mean=", avg), paste0("SD=", SD)),text.col=c("blue", "dark green"))
}
```

```{r, 1280, 10 @ mtcars, eval=FALSE, echo=FALSE}
# Plot the histogram of 1280 subsamples with 10 values in each subsample. 

means=meansVector(1280, 10, mtcars, 'mpg')
avg=round(mean(means), 2)
SD=round(sd(means), 2)
hist(means, xlim = c(15, 25), breaks=10, probability=TRUE,
     main = "10 subsamples with 10 values each")
lines(density(means, na.rm=TRUE), col="red")
  legend("topleft", legend = c(paste0("mean=", avg), paste0("SD=", SD)),text.col=c("blue", "dark green"))
```

```{r, 1280, 100 @ mtcars, eval=FALSE, echo=FALSE}
# gradually increase values taken in each subsample from 20-2560

x=1280
y=20*2^(c(0, seq(log2(2), log2(128))))

#par(mfrow=c(length(x),1),mar=rep(2, 4))
for (i in seq(1,length(y),by=1)){
  means=meansVector(x, y[i], mtcars, "mpg")
  avg=round(mean(means), 2)
  SD=round(sd(means), 2)
  hist(means, xlim = c(15, 25), breaks=20, probability=TRUE, 
       main = paste0(x, " subsamples with ", y[i], " values each"))
  lines(density(means, na.rm=TRUE), col="red")
  legend("topleft", legend = c(paste0("mean=", avg), paste0("SD=", SD)),text.col=c("blue", "dark green"))
}
```

# Confidence Interval (CI)

Confidence interval contains information about how the precision of the point estimate. We can use CI to make inference about the point estimate. However, when making statement about CI or interpreting CI, two things should be paying extra attention to:  

1. What are model parameters the CI is calculating for?  
2. What is the procedures being used to calculate CI?  

Resampling method is can be used to conveniently and intuitively calculate confidence interval (CI). That is to say, resampling method is a valid procedure to calculate CI. We will see the reampling method briefly in the t-test session and will have a whole session on the resampling method. 

First, a good model and the parameters in the model is the one that is similar to the true model, which generate the data. Of course we seldom have clue about what the true model is. Therefore we use various kind of method to find the "good" model. For example, the maximum likelihood method is one of the popular method to find a good model.  

Second, a valid procedure to calculate X% CI is the one that, when we have a good model and repeat the experiments many times and calculate CI each time, we should be see X% of the CI covers the true parameter value. In other word, we could say, if the model is good, there is X% of chance the CI would cover the true parameter value. Note that it does not mean X% CI will cover the true parameter value with X% chance. It does not, if the model is wrong at the beginning!

The interpretation of X% CI should be, if the model is good and the procedure to calculate CI is valid, the CI should covers the true parameter value with X% probability.  
Let's take an verbal example. We want to know the distribution of the heights of all Taiwanese high school students, so we did some survey and calculate the mean and variation of the survey. We might find that the survey data follows normal distribution, so we would assume the heights of all Taiwanese high school students follow normal distribution as well. Now, the model is a normal distribution (we want to use a normal distribution to describe the data) and the parameters to be estimated are mean and variance. Then based on the features of normal distribution and assumptions we made, we could use the standard error of the model parameters to calculate their confidence interval ([see here if you forgot why](https://en.wikipedia.org/wiki/Standard_error)). Using standard error of the estimated parameter value is the procedure.  

However, if the survey data is not a normal distribution, we can NOT use standard error to calculate the confidence interval of the mean. We will have to either figure out what distribution the survey data follows (which is often pretty challenging), or we can use resampling method! Note that, even when using the resampling method, we still have to justify if the normal distribution is a good model that describes the heights. 

## CI and SE

- [standard error (SE)](https://en.wikipedia.org/wiki/Standard_error): standard deviation of a statistic (e.g. mean). This is very different from the standard deviation of sample!  

- [confidence interval (CI)](https://en.wikipedia.org/wiki/Confidence_interval): the interval within which a statistic is expected to be observed. For example, "95% confidence interval of a statistic is X to Y" means that, if the __model is good enough__ and __a valid procedures__ is being used, we are 95% confident that the true value of the statistic is in X to Y. Upper/lower limit of 95% CI of the normal distribution mean is the mean plus/minus `r qnorm(0.975)` SE of the mean. [Why?](https://en.wikipedia.org/wiki/1.96)

I'll use the batting average data to demonstrate you the relationship between SE and CI. 

First we load the data and plot its distribution. 

```{r}
library(Lahman)
data(Batting)
bat15 = subset(Batting, yearID==2015 & AB>200)
bat15$avg = bat15$H/bat15$AB
hist(bat15$avg, breaks=30, freq=FALSE)
lines(density(bat15$avg), col="red")
shapiro.test(bat15$avg)
qqnorm(bat15$avg); qqline(bat15$avg, col="Red")
```

As this data follows normal distribution pretty well, so we can use SE of the mean to calculate the CI of the mean. The standard error of mean can be calculated as $\frac{SD}{\sqrt{n}}$, where _SD_ is the standard deviation and _n_ is sample size. The CI of the mean is plus/minus `r qnorm(0.975)` SE. 

```{r, echo=FALSE}
mean.CLT=mean(bat15$avg)
mean.SE.CLT=sqrt(var(bat15$avg)/length(bat15$avg))
mean.CI.CLT=c(mean.CLT-qnorm(0.975)*mean.SE.CLT, mean.CLT+qnorm(0.975)*mean.SE.CLT)
print(paste0("standard error = ", round(mean.SE.CLT,5)))
print(paste0("95% confidence interval : ", 
             round(mean.CLT-qnorm(0.975)*mean.SE.CLT,4), " - ", round(mean.CLT+qnorm(0.975)*mean.SE.CLT,4)))
```

Or, we can calculate CI by resampling method. This method is great as it is applicable to all kinds of sample distributions.

```{r}
set.seed(61)
avg.m=c()
for (i in 1:10000){
  avg.m[i]=mean(sample(bat15$avg, length(bat15$avg), replace=TRUE))
}

mean.resamp=mean(avg.m)
mean.SE.resamp=sd(avg.m)
mean.CI.resamp=c(sort(avg.m)[10000*0.025], sort(avg.m)[10000*0.975])
print(paste0("standard error calculated by resampling = ", round(mean.SE.resamp,5)))
print(paste0("95% confidence interval by resampling : ", 
             round(sort(avg.m)[10000*0.025],4), " - ", round(sort(avg.m)[10000*0.975],4)))
```

The CI calculated by two procedures are almost identical. 

How do they look on a figure?

```{r, echo=FALSE, fig.width=12}
xs = seq(0.25, 0.27, by =0.0001)
fA = dnorm(xs, mean=mean.CLT, sd=mean.SE.CLT)
plot(fA~xs, type="l", lwd=3, col="blue", xlim=c(0.25, 0.27), ylim=c(0, 250), 
     main="distribution of means", xlab="value", ylab="density")
abline(v=c(qnorm(0.025, mean=mean.CLT, sd=mean.SE.CLT), qnorm(0.975, mean=mean.CLT, sd=mean.SE.CLT)), col="blue", lty="dashed")
par(new=TRUE)

hist(avg.m, breaks=30, probability=TRUE, xlim=c(0.25, 0.27), ylim=c(0, 250), 
     main="", xlab="", ylab="")
lines(density(avg.m),col="red")
legend("topright", legend = c("distribution of mean based on the central limit theorm", 
                              "distribution of mean calculated by resampling"), 
       col=c("blue", "red"),
       text.col=c("blue", "red"),
       lty=c("dashed"), bty="n")
```

With the concept of SE and CI in mind, We can dig into the rational of comparing the mean of two sample in more detail.  
